{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAlgorithms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMN7fXOmSOYQXOwzFMJZH1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mina19/machine_learning_algorithms/blob/main/MLAlgorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUtKoKPo0QBc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "\n",
        "from sklern.metrics import accuracy_score, precision_score, recall_score\n",
        "from time import time\n",
        "\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oaz3v6Jlg7V1"
      },
      "source": [
        "mydata = pd.read_csv('../../../file.csv')\n",
        "\n",
        "features = pd.read_csv('../../filename.csv')\n",
        "labels = pd.read_csv('../../labels.csv')\n",
        "\n",
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPPPeHjl1JSo"
      },
      "source": [
        "1. Explore and clean the data.\n",
        "2. Split data into training, validation, and testing.\n",
        "3. Fit an initial model and evaluate.\n",
        "4. Tune hyperparameters using k-fold cross validation.\n",
        "5. Evaluate on validation set.\n",
        "6. Select best model and evaluate on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEm5L4pV0rZq"
      },
      "source": [
        "**Linear Regression**\n",
        "\n",
        "**Example:** Number of umbrellas sold depending on how much rain\n",
        "\n",
        "**Use when:** Continuous target variable\n",
        "\n",
        "**Don't use when:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psW6w-Bpws_I"
      },
      "source": [
        "# Drop all categorical features\n",
        "categorical_features = ['PassengerID', 'Name']\n",
        "mydata.drop(categorical_features, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1vlbpYgw4Ux"
      },
      "source": [
        "# Explore continuous features\n",
        "mydata.describe()\n",
        "\n",
        "# The count could reveal missing values.\n",
        "mydata.grouby('TargetLabel').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86svAlLXxk_q"
      },
      "source": [
        "# Missing at random? Or in a systematic way?\n",
        "mydata.groupby(mydata['Age'].isnull()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD5-W_rcx91a"
      },
      "source": [
        "# Plot continuous features\n",
        "for i in ['Feature1', 'Feature2']:\n",
        "  died = list(titanic[titanic['Survived'] == 0][i].dropna())\n",
        "  survived = list(titanic[titanic['Survived'] == 1][i].dropna())\n",
        "  xmin = min(min(died), min(survived))\n",
        "  xmax = max(max(died), max(survived))\n",
        "  width = (xmax - xmin) / 40\n",
        "  sns.distplot(died, color='r', kde=False, bins=np.arange(xmin, xmax, width))\n",
        "  sns.distplot(survived, color = 'g', kde = False, bins = np.arange(xmin, xmax, width))\n",
        "  plt.legend(['Did not survive', 'Survived'])\n",
        "  plt.title('Overlaid histograms for {}'.format(i))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06liUhw1yza9"
      },
      "source": [
        "for i, col in enumerate(['Pclass', 'SibSp', 'Parch']):\n",
        "    plt.figure(i)\n",
        "    sns.catplot(x=col, y='Survived', data=titanic, kind='point', aspect=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ysz6Q8z2k4b"
      },
      "source": [
        "Fill missing values as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcVJmJ7H2XZ9"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYZumnPr2tMC"
      },
      "source": [
        "df['Age'].fillna(df['Age'].mean(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWEoeALJ22lZ"
      },
      "source": [
        "sns.catplot(x=col, y='Survived', data=df, kind='point', aspect=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLblA7qB3mXk"
      },
      "source": [
        "df.drop(['A', 'B', 'C'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEiUsyVJ35o8"
      },
      "source": [
        "df.groupby(df['MissingCategory'].isnull())['y'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAc_2I2J4gEU"
      },
      "source": [
        "df['indicator'] = np.where(df['MissingCategory'].isnull(), 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mvRtJ0f4uSu"
      },
      "source": [
        "gender_num = {'male': 0, 'female': 1}\n",
        "df['gender'] = df['gender'].map(gender_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3A6t-vaz9Ce"
      },
      "source": [
        "# Explore categorical features\n",
        "mydata.info()\n",
        "\n",
        "for i, col in enumerate(['Pclass', 'SibSp', 'Parch']):\n",
        "    plt.figure(i)\n",
        "    sns.catplot(x=col, y='Survived', data=titanic, kind='point', aspect=2)\n",
        "\n",
        "df.pivot_table('Survived', index='Sex', columns='Embarked', aggfunc='count')\n",
        "df.pivot_table('Survived', index = 'Cabin_ind', columns = 'Embarked', aggfunct='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lHdYAsb5gC_"
      },
      "source": [
        "Split into training, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg0FYJ9Q5DV2"
      },
      "source": [
        "features = df.drop('y', axis=1)\n",
        "labels = df['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.4, random_state = 1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxV-ugJB6ksw"
      },
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "**Use when:** binary target variable, transparency is important or interested in significance of features, fairly well-behaved data, need a quick initial benchmark\n",
        "\n",
        "**Don't use when:** continuous target variable, massive amount of data (rows or columns), outliers or skewed features, performance is the only thing that matters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mOUL7zl6Egp"
      },
      "source": [
        "lr = LogisticRegression()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pcv9ut_80iu"
      },
      "source": [
        "C is a regularization paramter, default is 1.\n",
        "\n",
        "When C goes to infinity, large penalty for misclassification, more likely to overfit, lambda goes to zero (low regularization).\n",
        "\n",
        "When C goes to zero, small penalty for misclassification, more likely to underfit, lambda goes to infinity (high regularization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gStBry9fFF"
      },
      "source": [
        "**K-Fold Cross Validation**\n",
        "\n",
        "Dataset split into K subsets. Iterate through those K subsets K times. Fit model on K-1 subsets, test on remaining subset.\n",
        "Generate performance metric on each loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKcia2c69d3W"
      },
      "source": [
        "parameters ={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaTxi7r09F0f"
      },
      "source": [
        "cv = GridSearchCV(lr, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n",
        "cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1KfPWu_nHQ"
      },
      "source": [
        "**Support Vector Machines:** a classifier that finds an optimal hyperplane that maximizes the margin between two classes. Support vectors are the vector lines from the decision boundary to the closest points.\n",
        "\n",
        "A **kernel trick/method** transforms data that is not linearly separable in n-dimensional space to a higher dimension where it is linearly separable.\n",
        "\n",
        "**Use when:** Binary target variable, feature-to-row ratio is very high (short and fat data), very complex relationships, lots of outliers\n",
        "\n",
        "**Don't use when:** Feature-to-row is very low, transparency is important or interested in feature importance, looking for a quick benchmark model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCtu24X8-7m6"
      },
      "source": [
        "svc = SVC()\n",
        "parameters = {C = [0.1, 1, 10],\n",
        "              kernel = ['linear', 'rbf']}\n",
        "cv = GridSearchCV(svc, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n",
        "cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHxxnZ7_DDrA"
      },
      "source": [
        "**Multilayer Perceptron** is a classic feed-forward artificial neural network, the core component of deep learning. A connected series of nodes (in the form of a directed acyclic graph) where each node represents a function or a model.\n",
        "\n",
        "Input layer has one node for each features.\n",
        "Hidden layer with as many nodes as you want. Each node is a function.\n",
        "\n",
        "Output layer with a node for each possible number of outcome (or one node).\n",
        "\n",
        "**Use when:** categorical or continuous target variable, very complex relationships or performance is the only thing that matters, when control over the training process is important\n",
        "\n",
        "**Don't use when:** Transparency is important or interested in feature significance, need a quick benchmark model, limited data available\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjdvSCCRh8Ol"
      },
      "source": [
        "def print_results(results):\n",
        "  print('Best params: {}\\n'.format(results.best_params_))\n",
        "  means = results.cv_results_['mean_test_score']\n",
        "  stds = results.cv_results_['std_test_score']\n",
        "  for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
        "    print('{} (+/-{}) for {}'.format(round(mean, 3), round(std*2, 3), params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nau7EdHEj6J"
      },
      "source": [
        "mlp = MLPClassifier()\n",
        "parameters = {activation =['relu', 'tanh', 'logistic'],\n",
        "              hidden_layer_sizes = [(10,), (50,), (100,)],\n",
        "              learning_rate = ['constant', 'invscaling', 'adaptive']}\n",
        "cv = GridSearchCV(mlp, parameters, cv=5)\n",
        "cv.fit(X_train, y_train.values.ravel())\n",
        "print_results(cv)\n",
        "cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJc-JrwcFqrT"
      },
      "source": [
        "**Learning rate** hyperparameter facilitates both how quickly and whether or not the algorithm will find the optimal solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FipKPRzjOL3"
      },
      "source": [
        "**Random Forest** merges a collection of independent decision trees to get a more accurate and stable prediction. It is a type of ensemble method which combine several machine learning models in order to decrease both bias and variance.\n",
        "\n",
        "Take multiple data samples using sampling with replacement. Create feature samples for each data sample. Then, for each data and feature subset, create decision trees. For each example in set: run sample through the decision trees. Take the majority vote for final prediction.\n",
        "\n",
        "**Use when**: categorical or continuous target variable, interested in significance of predictors, need a quick benchmark model, if yoou have messy data with missing values or outliers\n",
        "\n",
        "**Don't use when**: if you're solving a very complex, novel problem, transparency is important, prediction time is important (quick to train but not quick for predictions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMLfgANOkvKB"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "parameters = {\n",
        "    'n_estimators' = [5, 50, 250],\n",
        "    'max_depth' = [2,4,8,16,32,None]}\n",
        "cv = GridSearchCV(rf, parameters, cv=5)\n",
        "cv.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "print_results(cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WLf45oPlKux"
      },
      "source": [
        "**n_estimators** hyperparameter controls how many individual decision trees will be built.\n",
        "\n",
        "**max_depth** hyperparameter controls how deep each individual decision tree can go.\n",
        "\n",
        "max_depth of 4 and n_estimators 50 should be decent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmULnstBmLgZ"
      },
      "source": [
        "**Boosting** is an ensemble method that aggregates a number of weak models to create one strong model.\n",
        "A weak model is one that is only slightly better than random guessing. A strong model is one that is strongly correlated with the true classification. In boosting, the decision trees are not independent.\n",
        "\n",
        "Boosting effectively learns from its mistakes with each iteration.\n",
        "\n",
        "Take first data sample and create a shallow decision tree. Evaluate its performance and overweight misclassified samples. Next model uses samples the first model couldn't quite figure out. It builds a new weak model. Repeat process again and again. By the end, you have n weak models that have learned from previous mistakes.\n",
        "\n",
        "For prediction, the models are parallelizable. Now you have weighted voting depending on how each model performed during training.\n",
        "\n",
        "Boosting is slow for fitting, but fast for prediction. It also has tendency to overfit.\n",
        "\n",
        "**Use when**: categorical or continuous target variable, useful on nearly any type of problem, interested in significance of predictors, prediction time is important\n",
        "\n",
        "**Don't use when**: transparency is important, training time is important or compute power is limited, data is really noisy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHLfuXKKoCQl"
      },
      "source": [
        "gb = GradientBoostingClassifier()\n",
        "parameters = {\n",
        "    'n_estimators': [5, 50, 250, 500],\n",
        "    'max_depth': [1,3,5,7,9],\n",
        "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "cv = GridSearchCV(gb, parameters, cv=5)\n",
        "cv.fit(X_train, y_train.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypkN0dnNqaBK"
      },
      "source": [
        "models = {}\n",
        "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
        "  models[mdl] = joblib.load('../../{}_model.pkl'.format(mdl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSxFkeKrhmh"
      },
      "source": [
        "def evaluate_model(name, model, features, labels):\n",
        "  start = time()\n",
        "  pred = model.predict(features)\n",
        "  end = time()\n",
        "  accuracy = round(accuracy_score(labels, pred), 3)\n",
        "  precision = round(precision_score, labels, pred), 3)\n",
        "  recall = round(recall_score(labels, pred), 3)\n",
        "  print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
        "                                                                                 accuracy,\n",
        "                                                                                 precision,\n",
        "                                                                                 recall, round((end - start),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWFS7oW3sJCE"
      },
      "source": [
        "for name, mdl in models.items():\n",
        "  evaluate_model(name, mdl, val_features, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zLDu9ussaYT"
      },
      "source": [
        "evaluate_model('Random Forest', models['RF'], te_features, te_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7WfxaJspPE"
      },
      "source": [
        "For spam detection problems, optimize for precision. If model says it's spam, it better be spam otherwise you miss real emails.\n",
        "\n",
        "For fraud detection, optimize for recall because missing any real fraudulent transactions could cost you a lot.\n",
        "\n",
        "If best model for overall accuracy is also slowest, might go for slightly less performing model with much lower latency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9tYcy5ql-cT"
      },
      "source": [
        "joblib.dump(cv.best_estimator_, '../../model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}